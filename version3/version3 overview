Version 3 Overview
In Version 3 of the project, we introduced new functionality with the addition of the /predict API endpoint. This version enhances the platform's ability to analyze and respond to system metrics using AI, providing predictive, prescriptive, and preventive insights based on real-time data from virtual machines.

Key Additions:
New API Endpoint:
http://127.0.0.1:5000/predict
This API is responsible for fetching virtual machine (VM) metrics and utilizing AI to analyze these metrics.

New Files:

predict.py: Handles the back-end logic of fetching metrics from the VM, passing these metrics to the AI model (Gemini), and generating meaningful predictions for users.
predict.html: A front-end component that displays the results and suggestions generated by the AI.
AI Implementation Approach:
1. Metrics Collection:
The process begins in predict.py, where we fetch real-time VM metrics by making a request to the Azure Monitor Metrics API. The metrics fetched include:

CPU Usage: Percentage of CPU utilization.
Available Memory: The remaining memory on the VM.
Network In/Out: The amount of network traffic in and out of the VM.
These metrics are obtained by hitting a URL endpoint that provides the current status of the virtual machine's performance.

2. Passing Metrics to Gemini Model:
Once the metrics are retrieved, they are formatted and passed to the Gemini AI model for analysis. The ChatGoogleGenerativeAI from the LangChain library is utilized to interact with the Gemini model. The metrics are fed into a carefully designed prompt that provides the AI with system details, such as CPU usage, available memory (converted to gigabytes), and network activity in a structured format.

3. AI Response:
The Gemini model processes these metrics and responds in three core ways:

Prediction: Anticipates future performance issues based on current trends, such as potential CPU overloads or memory exhaustion.
Prescription: Offers recommendations or actions to optimize system performance, such as increasing memory or adjusting workload distribution.
Prevention: Highlights any imminent risks that can lead to downtime or degraded performance, suggesting preventive measures to avoid such situations.
This functionality allows the system to not only analyze metrics but also provide insightful suggestions to improve or maintain the health of the virtual machine.

Steps to Setup:
Clone or Download the Project:
Obtain the project repository from GitHub or your chosen source.

Navigate to Your Project Directory:
Open a terminal or command prompt and navigate to the project directory.

Install the Necessary Dependencies:
Run the following command to install the required packages listed in the requirements.txt file:


pip install -r requirements.txt

Update the following configuration settings:
Change the service principal values in the configuration file.
Update the Gemini token to ensure proper authentication.

Start the Application:
Finally, run the application with the following command:
python app.py
This will start the server, and you can access the /predict endpoint to begin using the new functionality.

